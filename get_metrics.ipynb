{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import pickle\n",
    "from metrics import *\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "CATEGORIES_YAML = load_config(\"/hai/scratch/zwefers/seq2loc/metadata/location_levels.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv(\"/hai/scratch/zwefers/seq2loc/metadata/uniprot_trainset.csv\")\n",
    "testset = pd.read_csv(\"/hai/scratch/zwefers/seq2loc/metadata/hou_testset.csv\")\n",
    "outdir = \"outputs/seq2locbench/\"\n",
    "\n",
    "for model in [\"prott5\", \"esm1\"]:\n",
    "    val_avg_df = []\n",
    "    test_metrics_avg_df = []\n",
    "    for level in [1,2,3]:\n",
    "        val_perclass_df = []\n",
    "        test_targets_all = []\n",
    "        test_probs_all = []\n",
    "        test_preds_all = []\n",
    "\n",
    "        for fold in range(5):\n",
    "            val_path = f\"{outdir}/{model}/{fold}_1Layer_uniprot_level{level}_testout.pkl\"\n",
    "            val_df = pd.read_pickle(val_path)\n",
    "            val_df = val_df.merge(trainset, \n",
    "                                left_on=\"ACC\", \n",
    "                                right_on=\"uniprot_id\", \n",
    "                                how=\"inner\")\n",
    "            val_probs = np.stack(val_df.preds.to_numpy())\n",
    "            val_targets = []\n",
    "            for locs in val_df[f\"level{level}\"].str.split(\";\").to_list():\n",
    "                val_targets.append([1 if loc in locs else 0 \n",
    "                                    for loc in CATEGORIES_YAML[f\"level{level}\"]])\n",
    "            val_targets = np.array(val_targets)\n",
    "\n",
    "            thresholds = [get_best_threshold_mcc(val_targets[:, i], val_probs[:, i]) \n",
    "                        for i in range(val_targets.shape[1])]\n",
    "            thresholds = np.array(thresholds)\n",
    "            #TODO: save thresholds\n",
    "\n",
    "            _, val_metrics_perclass, val_metrics_avg = all_metrics(val_targets, \n",
    "                                                                val_probs, \n",
    "                                                                thresholds=thresholds)\n",
    "            val_metrics_perclass[\"label\"] = CATEGORIES_YAML[f\"level{level}\"]\n",
    "            val_metrics_perclass[\"fold\"] = fold\n",
    "            val_metrics_avg[\"level\"] = level\n",
    "            val_metrics_avg[\"fold\"] = fold\n",
    "            val_perclass_df.append(val_metrics_perclass)\n",
    "            val_avg_df.append(val_metrics_avg)\n",
    "\n",
    "            test_path = f\"{outdir}/{model}/{fold}_1Layer_uniprot_level{level}_hou_testout.pkl\"\n",
    "            test_df = pd.read_pickle(test_path)\n",
    "            test_df = test_df.merge(testset, \n",
    "                                    left_on=\"ACC\", \n",
    "                                    right_on=\"uniprot_id\",\n",
    "                                    how=\"inner\")\n",
    "            test_probs = np.stack(test_df.preds.to_numpy())\n",
    "            test_targets = []\n",
    "            for locs in test_df[f\"level{level}\"].str.split(\";\").to_list():\n",
    "                test_targets.append([1 if loc in locs else 0 \n",
    "                                    for loc in CATEGORIES_YAML[f\"level{level}\"]])\n",
    "            test_targets = np.array(test_targets)\n",
    "\n",
    "\n",
    "            test_preds = test_probs > thresholds[np.newaxis, :]\n",
    "            test_max = (test_probs == test_probs.max(axis=1)[:, np.newaxis])\n",
    "            test_preds = np.logical_or(test_preds, test_max)\n",
    "\n",
    "            test_targets_all.append(test_targets)\n",
    "            test_probs_all.append(test_probs)   \n",
    "            test_preds_all.append(test_preds)\n",
    "        \n",
    "        val_perclass_df = pd.concat(val_perclass_df)\n",
    "        val_perclass_df.to_csv(f\"{outdir}/{model}/val_metrics_perclass_level{level}.csv\", index=False)\n",
    "\n",
    "        test_targets_all = np.array(test_targets_all)\n",
    "        assert np.all(test_targets_all[0, :, :] == test_targets_all)\n",
    "        test_targets = test_targets_all[0, :, :]\n",
    "        test_probs = np.array(test_probs_all).mean(axis=0)\n",
    "        #test_preds = np.array(test_preds_all).max(axis=0)\n",
    "        test_preds = (np.array(test_preds_all).mean(axis=0) > 0.5).astype(np.int32)\n",
    "\n",
    "\n",
    "        #Cut out empty categories in testset like int-fils and plastid\n",
    "        idxs = np.where(test_targets.sum(axis=0) != 0)[0]\n",
    "        test_targets = test_targets[:, idxs]\n",
    "        test_probs = test_probs[:, idxs]\n",
    "        test_preds = test_preds[:, idxs]\n",
    "        thresholds = thresholds[idxs]\n",
    "\n",
    "\n",
    "        _, test_metrics_perclass, test_metrics_avg = all_metrics(\n",
    "                                                                test_targets, \n",
    "                                                                test_probs, \n",
    "                                                                y_pred_bin = test_preds,\n",
    "                                                                thresholds=thresholds\n",
    "                                                                )\n",
    "        test_metrics_perclass[\"label\"] = np.array(CATEGORIES_YAML[f\"level{level}\"])[idxs]\n",
    "        test_metrics_perclass.to_csv(\n",
    "            f\"{outdir}/{model}/test_metrics_perclass_level{level}.csv\", index=False)\n",
    "        \n",
    "        test_metrics_avg[\"level\"] = level\n",
    "        test_metrics_avg_df.append(test_metrics_avg)\n",
    "\n",
    "    val_avg_df = pd.concat(val_avg_df)\n",
    "    val_avg_df.to_csv(f\"{outdir}/{model}/val_metrics_avg.csv\", index=False)\n",
    "    test_metrics_avg_df = pd.concat(test_metrics_avg_df)\n",
    "    test_metrics_avg_df.to_csv(f\"{outdir}/{model}/test_metrics_avg.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mulocdeep_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
